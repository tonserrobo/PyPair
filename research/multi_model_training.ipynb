{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3efc613c-a38d-4b07-845f-4cb6d71b3970",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-06-02T23:16:55.540370800Z",
     "start_time": "2024-06-02T23:16:55.483686700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c57b1-22dc-469a-a60d-b7923a9abfe2",
   "metadata": {},
   "source": [
    "# Data Processesing\n",
    "This notebook segment outlines the initial steps of preparing a dataset for training a machine learning model, specifically focusing on location-based data. The code is divided into two main parts: data import and preparation of training and test data.\n",
    "\n",
    "## Data Import\n",
    "- The dataset is imported from a CSV file named `training_data.csv` using Pandas.\n",
    "- Any rows with missing values are dropped to ensure data quality and consistency. This is crucial for models that are sensitive to null values.\n",
    "- A specific preprocessing step is included to address an identified issue: rows where the 'org_location' field is 0.0 are removed. This step is noted as necessary due to a peculiar behavior of some models treating 0.0 as a null value.\n",
    "\n",
    "## Preparing Training and Test Data\n",
    "- The dataset is split into features (`x`) and the target variable (`y`), which is 'org_location' in this case.\n",
    "- The data is then divided into training and test sets using the `train_test_split` method from Scikit-Learn. 20% of the data is reserved for testing, and the split is performed with shuffling to ensure randomization, using a random state of 2 for reproducibility.\n",
    "- Further, the training set is split again to create a validation set, also using a 20% split with shuffling and the same random state. This additional split is important for model validation during training.\n",
    "- The training, test, and validation sets are saved into separate CSV files for easy access in later stages of the modeling process.\n",
    "\n",
    "The print statements at the end of each major section provide a quick summary of the dataset sizes, offering a clear understanding of how much data is available for training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changelog\n",
    "**V0.1**: Compleated the main operartion of the models including printing results to df. \n",
    "**V0.2**: Overfitting is a problem with many of the models. For others, the problem is very poor accruacy and memory overhead - changed the number of training samples, introduced SMOTE for class imbalance, Feature scaling and hyperparameter tunning with Gridsearch among other changes \n",
    "**V0.3**: Changed the git directory and cleaned this - research organsiation changes mostly\n",
    "**V0.4**: Changed the number of training samples to 10 (down from 100), removed the k-seed string (opting for runtime endoded and occruance counts only)\n",
    "**V0.5**: included SHAP in models to explore feature importance "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a540aabbae35b8e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Pharsing input reference genome\n",
      "\n",
      "pharsing reference sequence C:/Users/Tony/OneDrive - Ulster University (1)/Phd - FPGA acceleration in genomics/Publications + Writing/PhD_reports/PhD Thesis/cp3. Seed generation via ML/research/PyPair/reference_samples\\s1.fasta\n",
      "sequence ID: CM001012.3\n",
      "sequence Length: 10000\n",
      "Making rotations\n",
      "Building suffix array column\n",
      "Building BWT\n",
      "Populating FM index character counts\n",
      "Generating k-length seeds\n",
      "Cannot find reference_genomes directory, creating one...\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'data_sets'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\OneDrive - Ulster University (1)\\Phd - FPGA acceleration in genomics\\Publications + Writing\\PhD_reports\\PhD Thesis\\cp3. Seed generation via ML\\research\\PyPair\\gen_index.py:188\u001B[0m, in \u001B[0;36mGenIndex.generate_index\u001B[1;34m(self, reference_genome, seed_length)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 188\u001B[0m     index\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference_genomes/indexed_reference.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3893\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3894\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3895\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3899\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3900\u001B[0m )\n\u001B[1;32m-> 3902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameRenderer(formatter)\u001B[38;5;241m.\u001B[39mto_csv(\n\u001B[0;32m   3903\u001B[0m     path_or_buf,\n\u001B[0;32m   3904\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[0;32m   3905\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   3906\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   3907\u001B[0m     errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m   3908\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   3909\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n\u001B[0;32m   3910\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m   3911\u001B[0m     index_label\u001B[38;5;241m=\u001B[39mindex_label,\n\u001B[0;32m   3912\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m   3913\u001B[0m     chunksize\u001B[38;5;241m=\u001B[39mchunksize,\n\u001B[0;32m   3914\u001B[0m     quotechar\u001B[38;5;241m=\u001B[39mquotechar,\n\u001B[0;32m   3915\u001B[0m     date_format\u001B[38;5;241m=\u001B[39mdate_format,\n\u001B[0;32m   3916\u001B[0m     doublequote\u001B[38;5;241m=\u001B[39mdoublequote,\n\u001B[0;32m   3917\u001B[0m     escapechar\u001B[38;5;241m=\u001B[39mescapechar,\n\u001B[0;32m   3918\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   3919\u001B[0m )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1133\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1134\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1135\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1150\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1151\u001B[0m )\n\u001B[1;32m-> 1152\u001B[0m csv_formatter\u001B[38;5;241m.\u001B[39msave()\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    250\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    251\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,\n\u001B[0;32m    252\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression,\n\u001B[0;32m    253\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options,\n\u001B[0;32m    254\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    257\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    258\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    264\u001B[0m     )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:739\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[1;32m--> 739\u001B[0m     check_parent_directory(\u001B[38;5;28mstr\u001B[39m(handle))\n\u001B[0;32m    741\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:604\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[1;32m--> 604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: Cannot save file into a non-existent directory: 'reference_genomes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mFileExistsError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 123\u001B[0m\n\u001B[0;32m    120\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(folder_path, fasta_file)\n\u001B[0;32m    122\u001B[0m sequence_id, sequence_string, sequence_length \u001B[38;5;241m=\u001B[39m process_fasta_file(file_path, config)\n\u001B[1;32m--> 123\u001B[0m index \u001B[38;5;241m=\u001B[39m index_sequence(sequence_string, config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseed_length\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m    124\u001B[0m index_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(index)\n\u001B[0;32m    126\u001B[0m reverse_complement_df \u001B[38;5;241m=\u001B[39m reverse_complement(index_df) \u001B[38;5;66;03m# reverse compliment \u001B[39;00m\n",
      "Cell \u001B[1;32mIn[15], line 78\u001B[0m, in \u001B[0;36mindex_sequence\u001B[1;34m(_sequence_string, seed_length)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" Index sequence using GenIndex() \"\"\"\u001B[39;00m\n\u001B[0;32m     77\u001B[0m index_obj \u001B[38;5;241m=\u001B[39m GenIndex()\n\u001B[1;32m---> 78\u001B[0m _index \u001B[38;5;241m=\u001B[39m index_obj\u001B[38;5;241m.\u001B[39mgenerate_index(_sequence_string, seed_length)\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _index\n",
      "File \u001B[1;32m~\\OneDrive - Ulster University (1)\\Phd - FPGA acceleration in genomics\\Publications + Writing\\PhD_reports\\PhD Thesis\\cp3. Seed generation via ML\\research\\PyPair\\gen_index.py:191\u001B[0m, in \u001B[0;36mGenIndex.generate_index\u001B[1;34m(self, reference_genome, seed_length)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find reference_genomes directory, creating one...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 191\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_sets\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    192\u001B[0m     index\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference_genomes/indexed_reference.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index\n",
      "File \u001B[1;32m<frozen os>:225\u001B[0m, in \u001B[0;36mmakedirs\u001B[1;34m(name, mode, exist_ok)\u001B[0m\n",
      "\u001B[1;31mFileExistsError\u001B[0m: [WinError 183] Cannot create a file when that file already exists: 'data_sets'"
     ]
    }
   ],
   "source": [
    "\"\"\" This cell generates the fundamental index dataset within 'reference genomes' for the sample set. \"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from Bio.Seq import Seq\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"C:/Users/Tony/OneDrive - Ulster University (1)/Phd - FPGA acceleration in genomics/Publications + Writing/PhD_reports/PhD Thesis/cp3. Seed generation via ML/research\")\n",
    "\n",
    "from research.PyPair.io_processing import IO_processing\n",
    "from research.PyPair.gen_index import GenIndex\n",
    "\n",
    "config = {\n",
    "    \"seed_length\": 28,\n",
    "    \"read_length\": 100,\n",
    "    \"training_iterations\": 8,\n",
    "}\n",
    "\n",
    "\"\"\" Experiment specific functions \"\"\"\n",
    "def reverse_complement(sequence_set):\n",
    "    \"\"\" Compute reverse compliment of k-seed string \"\"\"\n",
    "    sequence_strings = sequence_set['k-seed']\n",
    "\n",
    "    rc_strings = []\n",
    "    clean_rc_strings = []\n",
    "    for string in sequence_strings:\n",
    "        s_seq = Seq(string)\n",
    "        rc_string = s_seq.reverse_complement()\n",
    "        rc_strings.append(rc_string)\n",
    "\n",
    "    for row in rc_strings:\n",
    "        new_string = ''.join(row)\n",
    "        clean_rc_strings.append(new_string)\n",
    "\n",
    "    sequence_set['rc_seeds'] = clean_rc_strings\n",
    "    return sequence_set\n",
    "\n",
    "def encode(_index):\n",
    "    \"\"\"encode the data set using 2bit encoding\"\"\"\n",
    "    encoded = []\n",
    "    dictionary = {'$': '', ',':'', 'A': '00', 'C': '01', 'G': '10', 'T': '11'}\n",
    "\n",
    "    if isinstance(_index, str):\n",
    "        transTable = _index.maketrans(dictionary)\n",
    "        txt = _index.translate(transTable)\n",
    "        encoded = txt\n",
    "    else:\n",
    "        for row in _index:\n",
    "            encoded_row = row.translate(row.maketrans(dictionary))\n",
    "            encoded.append(encoded_row)\n",
    "    return encoded\n",
    "\n",
    "def extend_dataset(_df, _config):\n",
    "    \"\"\" Extend training set with repeated instances of class example \"\"\"\n",
    "    classes = _df['org_location'].unique()\n",
    "    extended_df = pd.DataFrame()\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_df = _df[_df['org_location'] == cls]\n",
    "        repeat_times = max(1, _config['training_iterations'] - cls_df.shape[0])\n",
    "\n",
    "        extended_cls_df = pd.concat([cls_df] * repeat_times, ignore_index=True)\n",
    "        extended_df = pd.concat([extended_df, extended_cls_df], ignore_index=True)\n",
    "\n",
    "    return extended_df\n",
    "\n",
    "\"\"\" Data import and standard preprocessing functions \"\"\"\n",
    "def process_fasta_file(_file_path, _config):\n",
    "    \"\"\" Parse fasta file \"\"\"\n",
    "    io_processor = IO_processing()\n",
    "    _sequence_id, _sequence_string, _sequence_length = io_processor.pharse_reference(_file_path, _config['seed_length'])\n",
    "    return _sequence_id, _sequence_string, _sequence_length\n",
    "\n",
    "def index_sequence(_sequence_string, seed_length):\n",
    "    \"\"\" Index sequence using GenIndex() \"\"\"\n",
    "    index_obj = GenIndex()\n",
    "    _index = index_obj.generate_index(_sequence_string, seed_length)\n",
    "    return _index\n",
    "\n",
    "def save_index_as_csv(_index, file_name):\n",
    "    \"\"\" Save index function \"\"\"\n",
    "    _index_df = pd.DataFrame(_index)\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    _index_df.to_csv(file_name, index=False)\n",
    "\n",
    "def encode_dataframe(_df):\n",
    "    \"\"\" Encode dataframe \"\"\"\n",
    "    encoded_df = _df.copy()\n",
    "    for column in encoded_df.columns:\n",
    "        if column != 'org_location':\n",
    "            encoded_df[column] = encoded_df[column].apply(lambda _x: encode(_x) if isinstance(_x, str) else _x)\n",
    "    return encoded_df\n",
    "\n",
    "def runLengthEncoding(_input):\n",
    "    \"\"\" Run length encoding of k-seed string \"\"\"\n",
    "    _dict = OrderedDict.fromkeys(_input, 0)\n",
    "\n",
    "    for ch in _input:\n",
    "        _dict[ch] += 1\n",
    "    _output = ''\n",
    "    characters = ''\n",
    "    _values = ''\n",
    "    for key, _value in _dict.items():\n",
    "        _output = _output + key + str(_value)\n",
    "        characters = characters + key\n",
    "        _values = _values + str(_value)\n",
    "    return _output, characters, _values\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "remove all BWT aspects including occ array. Use only k-seed in mapping with rev compliment. If memory is a prob try with run length encoded params. \n",
    "\"\"\"\n",
    "\n",
    "folder_path = 'C:/Users/Tony/OneDrive - Ulster University (1)/Phd - FPGA acceleration in genomics/Publications + Writing/PhD_reports/PhD Thesis/cp3. Seed generation via ML/research/PyPair/reference_samples'\n",
    "fasta_files = [f for f in os.listdir(folder_path) if f.endswith('.fasta')]\n",
    "\n",
    "for i, fasta_file in enumerate(fasta_files, start=1):\n",
    "    file_path = os.path.join(folder_path, fasta_file)\n",
    "\n",
    "    sequence_id, sequence_string, sequence_length = process_fasta_file(file_path, config)\n",
    "    index = index_sequence(sequence_string, config['seed_length'])\n",
    "    index_df = pd.DataFrame(index)\n",
    "\n",
    "    reverse_complement_df = reverse_complement(index_df) # reverse compliment \n",
    "\n",
    "    # run length encoding \n",
    "    output = []\n",
    "    character = []\n",
    "    value = []\n",
    "\n",
    "    for seed_string in reverse_complement_df['k-seed']:\n",
    "        out, char, val = runLengthEncoding(seed_string)\n",
    "        output.append(out)\n",
    "        character.append(char)\n",
    "        value.append(val)\n",
    "\n",
    "    reverse_complement_df['runlength_output'] = output\n",
    "    reverse_complement_df['runlength_character'] = character\n",
    "    reverse_complement_df['runlength_value'] = value\n",
    "\n",
    "    del output\n",
    "    del character\n",
    "    del value\n",
    "\n",
    "    rc_output = []\n",
    "    rc_character = []\n",
    "    rc_value = []\n",
    "\n",
    "    for rc_seed_string in reverse_complement_df['rc_seeds']:\n",
    "        rc_out, rc_char, rc_val = runLengthEncoding(rc_seed_string)\n",
    "        rc_output.append(rc_out)\n",
    "        rc_character.append(rc_char)\n",
    "        rc_value.append(rc_val)\n",
    "\n",
    "    reverse_complement_df['runlength_rc_output'] = rc_output\n",
    "    reverse_complement_df['runlength_rc_character'] = rc_character\n",
    "    reverse_complement_df['runlength_rc_value'] = rc_value\n",
    "\n",
    "    del rc_output\n",
    "    del rc_character\n",
    "    del rc_value\n",
    "\n",
    "    # encode dataset \n",
    "    encoded_index = encode_dataframe(reverse_complement_df) #encodes strings \n",
    "\n",
    "    # filter columns based on training data analysis PCA \n",
    "    df_training_data = pd.DataFrame({\n",
    "        \"first\":encoded_index['first'],\n",
    "        \"A\":encoded_index['A'],\n",
    "        \"C\":encoded_index['C'],\n",
    "        \"G\":encoded_index['G'],\n",
    "        \"T\":encoded_index['T'],\n",
    "        # \"k-seed\":encoded_index['k-seed'],\n",
    "        \"rc_seeds\":encoded_index['rc_seeds'],\n",
    "        \"runlength_output\":encoded_index['runlength_output'],\n",
    "        \"runlength_value\":encoded_index['runlength_value'],\n",
    "        \"runlength_rc_output\":encoded_index['runlength_rc_output'],\n",
    "        \"runlength_rc_character\":encoded_index['runlength_rc_character'],\n",
    "        \"runlength_rc_value\":encoded_index['runlength_rc_value'],\n",
    "        \"org_location\":encoded_index['org_location'],\n",
    "    })\n",
    "\n",
    "    # Extend the dataset\n",
    "    extended_encoded_index = extend_dataset(df_training_data, config)\n",
    "\n",
    "    # Save the extended encoded index\n",
    "    extended_output_file_name = os.path.join(folder_path, f'S{i}_extended_encoded_reference.csv')\n",
    "    save_index_as_csv(extended_encoded_index, extended_output_file_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T23:16:55.712151700Z",
     "start_time": "2024-06-02T23:16:55.492729700Z"
    }
   },
   "id": "2da3dfce088320c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66569299-5923-42c3-94cc-475d9a88d392",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = 'reference_samples'\n",
    "reference_files = [f for f in os.listdir(folder_path) if f.endswith('extended_encoded_reference.csv')]\n",
    "\n",
    "for ref_file in reference_files:\n",
    "    # Data import\n",
    "    file_path = os.path.join(folder_path, ref_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna()\n",
    "    df = df[df['org_location'] != 0.0]  # Handle the error with models treating 0.0 as null\n",
    "    #df = df.drop(columns=['suffix_array', 'k-seed-extend'])\n",
    "\n",
    "    # Preparing training and test data\n",
    "    x = df.loc[:, df.columns != 'org_location']\n",
    "    y = df.loc[:, 'org_location']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    # Saving the datasets in the same folder\n",
    "    base_filename = ref_file.split('_reference.csv')[0]\n",
    "    X_train.to_csv(os.path.join(folder_path, f\"{base_filename}_x_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(folder_path, f\"{base_filename}_X_test.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(folder_path, f\"{base_filename}_y_train.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(folder_path, f\"{base_filename}_y_test.csv\"), index=False)\n",
    "    X_val.to_csv(os.path.join(folder_path, f\"{base_filename}_X_val.csv\"), index=False)\n",
    "    y_val.to_csv(os.path.join(folder_path, f\"{base_filename}_y_val.csv\"), index=False)\n",
    "\n",
    "    print(f\"Processed {ref_file}:\")\n",
    "    print(f\"  X training dataset length: {len(X_train)}\")\n",
    "    print(f\"  y training dataset length: {len(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "This section of the notebook focuses on the training of multiple machine learning models using Scikit-Learn version 0.24.2. The models selected represent different archetypes in machine learning, including Nearest Neighbour, Averaging Methods, Naïve Bayes, and Gradient Boosting. The specific models and their roles are as follows:\n",
    "\n",
    "## Models Overview\n",
    "\n",
    "1. **Nearest Centroid (Nearest Neighbour Archetype)**: \n",
    "   - This model is a simplistic yet effective approach for classification, based on the concept of the nearest centroid. It can be particularly useful for baseline comparisons.\n",
    "\n",
    "2. **KNeighbours Classifier (Nearest Neighbour Archetype)**: \n",
    "   - A versatile and widely-used model that classifies data based on the closest training examples in the feature space. It's effective for datasets where the decision boundary is irregular.\n",
    "\n",
    "3. **Random Forest (Averaging Methods Archetype)**: \n",
    "   - A robust ensemble learning method, Random Forest constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) of the individual trees.\n",
    "\n",
    "4. **Extra Trees Classifier (Averaging Methods Archetype)**: \n",
    "   - Similar to Random Forest, this model fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve predictive accuracy and control over-fitting.\n",
    "\n",
    "5. **Decision Tree (Averaging Methods Archetype)**: \n",
    "   - A decision tree is a flowchart-like structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
    "\n",
    "6. **Gaussian Naïve Bayes (Naïve Bayes Archetype)**: \n",
    "   - This model applies the Bayes theorem with the “naive” assumption of independence between every pair of features. Gaussian Naïve Bayes is particularly suited when the features have continuous values.\n",
    "\n",
    "7. **LGBM Classifier (Gradient Boosting Archetype)**: \n",
    "   - Light Gradient Boosting Machine is a gradient boosting framework that uses tree-based learning algorithms. It's designed for distributed and efficient training, particularly on large datasets.\n",
    "\n",
    "8. **XGB Classifier (Gradient Boosting Archetype)**: \n",
    "   - XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. It is highly efficient, flexible, and portable, often delivering state-of-the-art performance in many machine learning tasks.\n",
    "\n",
    "## Training Process\n",
    "Each model is trained on the prepared dataset, and the performance is evaluated using the validation set. The training involves tuning model parameters to find the optimal configuration for each model. Key metrics like accuracy, precision, recall, and F1 score are used to assess each model's performance. The diversity in the selected models ensures a comprehensive examination of the dataset, as each model type brings its strengths and weaknesses to different types of data. This approach allows for a thorough understanding of which models are best suited for the specific characteristics of the dataset in question.The results from these models can be used to benchmark performance and guide the selection of the most suitable model for deployment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bca45a8da1be88db"
  },
  {
   "cell_type": "markdown",
   "id": "8a54b8ea-1346-4f8d-864d-95dac9e2ccfb",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Nearest Neighbour\n",
    "Nearest Neighbor models are a fundamental class of algorithms in the field of machine learning, predominantly used for classification tasks, though they can also be employed for regression. These models operate on the principle of similarity, identifying the closest data points in the training set to make predictions for new, unseen data. The simplicity, intuitiveness, and effectiveness of these models make them an essential part of any data scientist's toolkit.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Basic Principle**: The core idea behind nearest neighbor models is that similar data points are close to each other in the feature space. Therefore, the label or value of a new data point can be predicted based on the labels or values of its nearest neighbors in the training set.\n",
    "\n",
    "2. **Distance Metrics**: These models rely on distance metrics to determine the closeness of data points. Common metrics include Euclidean, Manhattan, and Hamming distances. The choice of metric can significantly impact the model's performance and is often dictated by the nature of the data.\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN)**: One of the most popular variants is the K-Nearest Neighbors algorithm. KNN uses a predefined number 'K' to determine the number of neighboring data points to consider for making a prediction. The optimal value of 'K' is typically selected through cross-validation.\n",
    "\n",
    "4. **Nearest Centroid Classifier**: This variant classifies data points based on the closest centroid of the training samples in the feature space. It's particularly useful when the data is well-separated into clusters.\n",
    "\n",
    "5. **Applications**: Nearest neighbor models are used in a wide range of applications, including image recognition, recommendation systems, and medical diagnosis, where the assumption that similar instances have similar outcomes holds true.\n",
    "\n",
    "6. **Strengths and Limitations**:\n",
    "   - **Strengths**: These models are easy to implement, interpret, and don't require assumptions about the underlying data distribution. They're also highly adaptable to changes in the input data.\n",
    "   - **Limitations**: Nearest neighbor models can suffer from high computational costs, especially with large datasets, and can be sensitive to irrelevant or redundant features.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "In practical scenarios, implementing nearest neighbor models involves careful preprocessing of data, selection of an appropriate distance metric, and tuning of parameters like 'K' in KNN. It's also crucial to scale or normalize the data, as these models are sensitive to the scale of the input features.\n",
    "\n",
    "Moreover, modern applications might require optimizations for handling large datasets, such as using approximate nearest neighbor search algorithms or efficient data structures like KD-trees and Ball Trees.\n",
    "\n",
    "In summary, nearest neighbor models are a versatile and straightforward tool for both classification and regression tasks in machine learning. Their ability to adapt to complex, real-world datasets makes them a valuable component of the machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "S1_X_train = pd.read_csv('reference_samples/S1_extended_encoded_X_train.csv')\n",
    "S1_X_test = pd.read_csv('reference_samples/S1_extended_encoded_X_test.csv')\n",
    "S1_y_train = pd.read_csv('reference_samples/S1_extended_encoded_y_train.csv')\n",
    "S1_y_test = pd.read_csv('reference_samples/S1_extended_encoded_y_test.csv')\n",
    "\n",
    "S2_X_train = pd.read_csv('reference_samples/S2_extended_encoded_X_train.csv')\n",
    "S2_X_test = pd.read_csv('reference_samples/S2_extended_encoded_X_test.csv')\n",
    "S2_y_train = pd.read_csv('reference_samples/S2_extended_encoded_y_train.csv')\n",
    "S2_y_test = pd.read_csv('reference_samples/S2_extended_encoded_y_test.csv')\n",
    "\n",
    "S3_X_train = pd.read_csv('reference_samples/S3_extended_encoded_X_train.csv')\n",
    "S3_X_test = pd.read_csv('reference_samples/S3_extended_encoded_X_test.csv')\n",
    "S3_y_train = pd.read_csv('reference_samples/S3_extended_encoded_y_train.csv')\n",
    "S3_y_test = pd.read_csv('reference_samples/S3_extended_encoded_y_test.csv')\n",
    "\n",
    "S4_X_train = pd.read_csv('reference_samples/S4_extended_encoded_X_train.csv')\n",
    "S4_X_test = pd.read_csv('reference_samples/S4_extended_encoded_X_test.csv')\n",
    "S4_y_train = pd.read_csv('reference_samples/S4_extended_encoded_y_train.csv')\n",
    "S4_y_test = pd.read_csv('reference_samples/S4_extended_encoded_y_test.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "91e7614346ef1e2d"
  },
  {
   "cell_type": "markdown",
   "id": "1aa073ab-dbb5-4e87-8fa0-dcbd00967e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nearest Centroid\n",
    "Nearest Centroid model classifies data by assigning each observation to the class of the nearest centroid, simplifying computation and making it particularly effective for large, well-separated datasets.\n",
    "\n",
    "- **Ensuring Numeric Data**:\n",
    "  - **Converting Data to Numeric**: We use `pd.to_numeric` to convert all feature columns to numeric types, coercing non-numeric values to NaN. This helps standardize the data and ensure consistency.\n",
    "  - **Reference**: [Pandas to_numeric Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html)\n",
    "\n",
    "- **Handling NaN Values**:\n",
    "  - **Forward-Filling and Dropping NaNs**: NaN values are filled using the forward-fill method (`fillna(method='ffill')`), and any remaining NaNs are dropped using `dropna()`. This helps in cleaning the data and avoiding issues during model training.\n",
    "  - **Reference**: [Pandas fillna Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
    "\n",
    "- **Handling Infinite Values**:\n",
    "  - **Replacing Infinite Values with NaNs**: Infinite values are replaced with NaNs (`replace([np.inf, -np.inf], np.nan)`), and these NaNs are subsequently handled using forward-filling and dropping. This ensures that the dataset does not contain values that could cause computational issues.\n",
    "  - **Reference**: [Numpy isinf Documentation](https://numpy.org/doc/stable/reference/generated/numpy.isinf.html)\n",
    "\n",
    "- **Feature Scaling**:\n",
    "  - **StandardScaler**: We use `StandardScaler` to normalize the features to have a mean of 0 and a standard deviation of 1. This helps in improving the performance of many machine learning algorithms.\n",
    "  - **Reference**: [Sklearn StandardScaler Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- **Handling Class Imbalance**:\n",
    "  - **SMOTE (Synthetic Minority Over-sampling Technique)**: SMOTE is used to oversample the minority class by generating synthetic samples. This helps in addressing class imbalance and improving the model's ability to generalize.\n",
    "  - **Reference**: [Imbalanced-learn SMOTE Documentation](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)\n",
    "\n",
    "- **Hyperparameter Tuning**:\n",
    "  - **GridSearchCV**: We use `GridSearchCV` to perform an exhaustive search over specified hyperparameter values. This helps in finding the best model parameters and improving model performance.\n",
    "  - **Reference**: [Sklearn GridSearchCV Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "- **Cross-Validation**:\n",
    "  - **StratifiedShuffleSplit**: This cross-validation strategy ensures that each fold of the cross-validation split preserves the percentage of samples for each class, providing a robust evaluation of model performance.\n",
    "  - **Reference**: [Sklearn StratifiedShuffleSplit Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n",
    "\n",
    "These methods collectively help in cleaning the data, handling class imbalance, tuning model parameters, and ultimately addressing overfitting, leading to a more robust and generalizable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a561d-2570-48e4-b3b8-146d1818cdb0",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "import shap\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "def profile_memory(_model, _X_test):\n",
    "    try:\n",
    "        _mem_usage = memory_usage((_model.predict, (_X_test,)), interval=0.1)\n",
    "        mem_usage = max(_mem_usage) - min(_mem_usage)\n",
    "        if mem_usage == 0:\n",
    "            mem_usage = np.nan\n",
    "    except NotFittedError:\n",
    "        mem_usage = np.nan\n",
    "    return mem_usage\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "    y_train = pd.Series(y_train).apply(pd.to_numeric, errors='coerce')\n",
    "    y_test = pd.Series(y_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Handle NaN values\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    y_train = pd.Series(y_train).ffill().dropna().values\n",
    "    y_test = pd.Series(y_test).ffill().dropna().values\n",
    "\n",
    "    # Handle infinite values\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y_train = np.where(np.isinf(y_train), np.nan, y_train)\n",
    "    y_test = np.where(np.isinf(y_test), np.nan, y_test)\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    y_train = pd.Series(y_train).ffill().dropna().values\n",
    "    y_test = pd.Series(y_test).ffill().dropna().values\n",
    "\n",
    "    # Ensure data consistency\n",
    "    feature_names = X_train.columns.tolist()  # Save column names for SHAP\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train_scaled = np.vstack([X_train_scaled, X_train_scaled[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'metric': ['manhattan', 'euclidean'],\n",
    "        'shrink_threshold': [None, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    model = NearestCentroid()\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Measure memory usage during prediction\n",
    "    mem_usage = profile_memory(best_model, X_test_scaled)\n",
    "\n",
    "    # Measure execution time during prediction\n",
    "    start_time = time.time()\n",
    "    predicted = best_model.predict(X_test_scaled)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test_scaled)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test_scaled) if not np.isnan(mem_usage) else np.nan  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save model weights (centroids)\n",
    "filename = 'model_weights/centroid_model.joblib'\n",
    "\n",
    "if hasattr(best_model, 'centroids_'):\n",
    "    model_weights = best_model.centroids_\n",
    "    joblib.dump(model_weights, filename)\n",
    "else:\n",
    "    print(\"The model does not have the attribute 'centroids_'\")\n",
    "    joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.KernelExplainer(best_model.predict, X_train_resampled[:100])  # Use a subset of the training data\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NearestCentroid_shap_values = pd.DataFrame(shap_values)\n",
    "NearestCentroid_shap_values['Classifier'] = 'NearestCentroid'\n",
    "NearestCentroid_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "e12689e0f32db1e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NearestCentroid_results = pd.DataFrame(aggregate_metrics)\n",
    "NearestCentroid_results['Model'] = 'Nearest Centroid'\n",
    "NearestCentroid_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "NearestCentroid_results = NearestCentroid_results[['Model',\n",
    "                                   'Sample',\n",
    "                                   'Mean CV F1 Score',\n",
    "                                   'Precision',\n",
    "                                   'Recall',\n",
    "                                   'F1 Score',\n",
    "                                   'Memory (MB) per read mapping',\n",
    "                                   'Execution Time (s) per read mapping',\n",
    "                                   'Memory (MB) 50k batch',\n",
    "                                   'Execution Time (s) 50k batch']]\n",
    "NearestCentroid_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "f59f169ebb9261b4"
  },
  {
   "cell_type": "markdown",
   "id": "61c4dece-1ac5-4879-b24e-f40f9e2fcfb0",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour\n",
    "The Nearest Neighbor algorithm is a simple and intuitive classification method that predicts the label of a new data point based on the most common label among its closest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e099db-8eb1-4106-afa1-f02737aebf11",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "import shap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "def profile_memory(_model, _X_test):\n",
    "    _mem_usage = memory_usage((_model.predict, (_X_test,)), interval=0.1)\n",
    "    return max(_mem_usage) - min(_mem_usage)\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Ensure all data is numeric\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "    y_train = pd.Series(y_train).apply(pd.to_numeric, errors='coerce')\n",
    "    y_test = pd.Series(y_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Handle NaN values\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    y_train = pd.Series(y_train).ffill().dropna().values\n",
    "    y_test = pd.Series(y_test).ffill().dropna().values\n",
    "\n",
    "    # Handle infinite values\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y_train = np.where(np.isinf(y_train), np.nan, y_train)\n",
    "    y_test = np.where(np.isinf(y_test), np.nan, y_test)\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    y_train = pd.Series(y_train).ffill().dropna().values\n",
    "    y_test = pd.Series(y_test).ffill().dropna().values\n",
    "\n",
    "    # Ensure data consistency\n",
    "    feature_names = X_train.columns.tolist()  # Save column names for SHAP\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train_scaled = np.vstack([X_train_scaled, X_train_scaled[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'metric': ['minkowski', 'euclidean', 'manhattan'],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Measure memory usage during prediction\n",
    "    try:\n",
    "        mem_usage = profile_memory(best_model, X_test_scaled)\n",
    "        if mem_usage == 0:\n",
    "            mem_usage = np.nan  # Ensure no zero values\n",
    "    except NotFittedError:\n",
    "        mem_usage = np.nan\n",
    "\n",
    "    # Measure execution time during prediction\n",
    "    start_time = time.time()\n",
    "    predicted = best_model.predict(X_test_scaled)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test_scaled)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test_scaled) if not np.isnan(mem_usage) else np.nan  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained KNN model\n",
    "filename = 'model_weights/knn_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.KernelExplainer(best_model.predict, X_train_resampled[:100])  # Use a subset of the training data\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KNeighborsClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "KNeighborsClassifier_shap_values['Classifier'] = 'KNeighborsClassifier'\n",
    "KNeighborsClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "d215e6fe49326460"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KNeighborsClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "KNeighborsClassifier_results['Model'] = 'KNeighborsClassifier'\n",
    "KNeighborsClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "KNeighborsClassifier_results = KNeighborsClassifier_results[['Model',\n",
    "                                                   'Sample',\n",
    "                                                   'Mean CV F1 Score',\n",
    "                                                   'Precision',\n",
    "                                                   'Recall',\n",
    "                                                   'F1 Score',\n",
    "                                                   'Memory (MB) per read mapping',\n",
    "                                                   'Execution Time (s) per read mapping',\n",
    "                                                   'Memory (MB) 50k batch',\n",
    "                                                   'Execution Time (s) 50k batch']]\n",
    "KNeighborsClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "dbb6b87b5a90eab2"
  },
  {
   "cell_type": "markdown",
   "id": "dbac670d-6178-44e8-82c5-e9c7a329048c",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees, thereby enhancing predictive accuracy and robustness against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a6e7c-46b5-4600-8000-1b22a36e736a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import joblib\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Replace NaNs and infinities\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=1000):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Ensure data is in float32 format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(criterion='gini')\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=1000)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained RandomForest model\n",
    "filename = 'model_weights/random_forest_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "RandomForestClassifier_shap_values['Classifier'] = 'RandomForestClassifier'\n",
    "RandomForestClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.634449800Z"
    }
   },
   "id": "4b8728d40b9ea906"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "RandomForestClassifier_results['Model'] = 'RandomForestClassifier'\n",
    "RandomForestClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "RandomForestClassifier_results = RandomForestClassifier_results[['Model',\n",
    "                                                             'Sample',\n",
    "                                                             'Mean CV F1 Score',\n",
    "                                                             'Precision',\n",
    "                                                             'Recall',\n",
    "                                                             'F1 Score',\n",
    "                                                             'Memory (MB) per read mapping',\n",
    "                                                             'Execution Time (s) per read mapping',\n",
    "                                                             'Memory (MB) 50k batch',\n",
    "                                                             'Execution Time (s) 50k batch']]\n",
    "RandomForestClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.641979100Z"
    }
   },
   "id": "c52e8a15cd287ed1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Aggregating NearestNeighbour results '''\n",
    "NearestNeighbour_results = pd.concat([NearestCentroid_results, \n",
    "                                      KNeighborsClassifier_results, \n",
    "                                      RandomForestClassifier_results])\n",
    "NearestNeighbour_shap_values = pd.concat([NearestCentroid_shap_values,\n",
    "                                          KNeighborsClassifier_shap_values,\n",
    "                                          RandomForestClassifier_shap_values])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.641979100Z"
    }
   },
   "id": "32690b9b4339e35d"
  },
  {
   "cell_type": "markdown",
   "id": "e4d921fc-c8e0-49dd-ba29-3f11594f345b",
   "metadata": {},
   "source": [
    "## Averaging Methods\n",
    "Averaging Methods, including algorithms like Random Forest and Ensemble Methods, are a critical class of machine learning models known for their robustness and accuracy. These models work by combining the predictions from multiple models to improve the overall performance, especially in terms of variance reduction.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Basic Principle**: Averaging methods involve training multiple models and combining their predictions. The final output is typically the mean (for regression tasks) or mode (for classification tasks) of the predictions from all models. This approach helps in mitigating the effects of overfitting and improving the model's generalization capabilities.\n",
    "\n",
    "2. **Random Forest**: A popular example of averaging methods, Random Forest builds numerous decision trees and merges their outcomes. It is a form of 'bagging' where each tree is trained on a subset of the data and features, offering a diversified model performance.\n",
    "\n",
    "3. **Ensemble Learning**: Averaging methods are a subset of ensemble learning, where the objective is to combine the strengths of various models to achieve better accuracy and stability. This includes methods like bagging and boosting.\n",
    "\n",
    "4. **Boosting**: Another form of ensemble learning where models are trained sequentially with each model learning from the errors of its predecessors, thus focusing more on the challenging parts of the dataset.\n",
    "\n",
    "5. **Applications**: These methods are extremely versatile and have been successfully applied in various domains, such as finance for risk assessment, healthcare for disease prediction, and natural language processing tasks.\n",
    "\n",
    "6. **Strengths and Limitations**:\n",
    "   - **Strengths**: Averaging methods are known for their high accuracy, ability to handle large datasets and feature spaces, and robustness against overfitting. They also work well with non-linear data.\n",
    "   - **Limitations**: These models can be complex, computationally intensive, and less interpretable compared to simpler models like linear regression or decision trees.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing averaging methods requires selecting the right base models and determining how to combine their predictions effectively. The choice of base models and the method of combination (like voting or averaging) can significantly impact the performance. It is also crucial to ensure diversity among the base models to maximize the benefits of averaging.\n",
    "\n",
    "Additionally, hyperparameter tuning plays a significant role in optimizing these models. Techniques like cross-validation are essential for determining the optimal settings for parameters like the number of trees in a Random Forest or the learning rate in boosting algorithms.\n",
    "\n",
    "In conclusion, averaging methods are a powerful set of tools in the machine learning arsenal, offering enhanced predictive performance and robustness. Their ability to combine multiple models' strengths makes them suitable for a wide range of complex real-world problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dcf3a-69b9-4d80-84bf-c9316a85ee34",
   "metadata": {},
   "source": [
    "### Extra Trees classifier\n",
    "\n",
    "The Extra Trees Classifier is an ensemble machine learning algorithm that operates similarly to a Random Forest but with randomization at the level of individual tree splits, offering increased variance reduction and potentially faster training at the cost of slightly higher bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e0655-b30e-4a77-b24b-b61dc510d846",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.641979100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import joblib\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Replace NaNs, infinities, and too large values\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    X_train.ffill(inplace=True)\n",
    "    X_train.dropna(inplace=True)\n",
    "    X_test.ffill(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=500):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Ensure data is in float32 format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    model = ExtraTreesClassifier()\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=500)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained Extra Trees model\n",
    "filename = 'model_weights/extra_trees_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ExtraTreesClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "ExtraTreesClassifier_shap_values['Classifier'] = 'ExtraTreesClassifier'\n",
    "ExtraTreesClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "791cd594945c0b3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ExtraTreesClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "ExtraTreesClassifier_results['Model'] = 'ExtraTreesClassifier'\n",
    "ExtraTreesClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "ExtraTreesClassifier_results = ExtraTreesClassifier_results[['Model',\n",
    "                                                                 'Sample',\n",
    "                                                                 'Mean CV F1 Score',\n",
    "                                                                 'Precision',\n",
    "                                                                 'Recall',\n",
    "                                                                 'F1 Score',\n",
    "                                                                 'Memory (MB) per read mapping',\n",
    "                                                                 'Execution Time (s) per read mapping',\n",
    "                                                                 'Memory (MB) 50k batch',\n",
    "                                                                 'Execution Time (s) 50k batch']]\n",
    "ExtraTreesClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "47e6ee2d996d9fc0"
  },
  {
   "cell_type": "markdown",
   "id": "9d8d9ab7-c123-42d3-a2cc-5cb1fec70011",
   "metadata": {},
   "source": [
    "### Decision trees classifier\n",
    "The Decision Trees Classifier is a versatile and interpretable machine learning algorithm that classifies data by splitting it based on feature values, creating a tree-like model of decisions and their possible consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961d957-11ae-4cee-9d74-d9245ec93d90",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import joblib\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Convert all values to numerical types, replacing non-numerical values with NaN\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Replace NaNs, infinities, and too large values\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=500):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Accuracy': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Ensure data is in float32 format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=500)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Accuracy'].append(accuracy)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained Decision Tree model\n",
    "filename = 'model_weights/decision_tree_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DecisionTreeClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "DecisionTreeClassifier_shap_values['Classifier'] = 'DecisionTreeClassifier'\n",
    "DecisionTreeClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "bcaadbbfe54f30ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DecisionTreeClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "DecisionTreeClassifier_results['Model'] = 'DecisionTreeClassifier'\n",
    "DecisionTreeClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "DecisionTreeClassifier_results = DecisionTreeClassifier_results[['Model',\n",
    "                                                             'Sample',\n",
    "                                                             'Mean CV F1 Score',\n",
    "                                                             'Precision',\n",
    "                                                             'Recall',\n",
    "                                                             'F1 Score',\n",
    "                                                             'Memory (MB) per read mapping',\n",
    "                                                             'Execution Time (s) per read mapping',\n",
    "                                                             'Memory (MB) 50k batch',\n",
    "                                                             'Execution Time (s) 50k batch']]\n",
    "DecisionTreeClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "23d06ec037a20ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Aggregating AveragingMethods results '''\n",
    "AveragingMethods_results = pd.concat([ExtraTreesClassifier_results,\n",
    "                                      DecisionTreeClassifier_results])\n",
    "AveragingMethods_shap_values = pd.concat([ExtraTreesClassifier_shap_values,\n",
    "                                          DecisionTreeClassifier_shap_values])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "4832efe026da646f"
  },
  {
   "cell_type": "markdown",
   "id": "d5d17a83-820c-4494-b819-9ecdeabc3f3d",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "Naïve Bayes algorithms represent a family of simple yet effective probabilistic classifiers based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. Widely used in various applications, they are particularly known for their efficiency and ease of implementation.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Bayesian Theory**: The core principle of Naïve Bayes is Bayes' theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature.\n",
    "\n",
    "2. **Feature Independence**: Naïve Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. This assumption simplifies the computation, hence the term \"naïve.\"\n",
    "\n",
    "3. **Variants of Naïve Bayes**:\n",
    "   - **Gaussian Naïve Bayes**: Assumes that the continuous values associated with each class are distributed according to a Gaussian distribution.\n",
    "   - **Multinomial Naïve Bayes**: Typically used for document classification, where the features are the frequencies of the words or tokens.\n",
    "   - **Bernoulli Naïve Bayes**: Used in binary classification, especially text classification with 'bag of words' model.\n",
    "\n",
    "4. **Applications**: Naïve Bayes classifiers are widely used in spam filtering, sentiment analysis, and document classification. They are also employed in medical diagnosis and weather prediction.\n",
    "\n",
    "5. **Strengths and Limitations**:\n",
    "   - **Strengths**: They are easy to implement, can handle both continuous and discrete data, and perform well in multi-class prediction. When the independence assumption holds, a Naïve Bayes classifier performs better compared to other models and requires much less training data.\n",
    "   - **Limitations**: Their strong feature independence assumptions can lead to poor performance if this assumption does not hold. In practice, they are often outperformed by models like Random Forest or Gradient Boosting.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing Naïve Bayes models involves careful preprocessing of data. For text data, techniques like bag-of-words or TF-IDF are common. Feature scaling is not required as the classifiers are not sensitive to the magnitude of data. Tuning involves choosing the right variant of Naïve Bayes and adjusting parameters like the smoothing factor in Multinomial and Bernoulli Naïve Bayes.\n",
    "\n",
    "In summary, Naïve Bayes classifiers, with their basis in probability theory, offer a straightforward and efficient approach for building fast and scalable machine learning models. Their simplicity and the ability to make probabilistic predictions make them useful, especially in the initial stages of a modeling pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a98b3-76bc-4108-8680-4debc68e0cd9",
   "metadata": {},
   "source": [
    "## Gaussian naive bayes classifier\n",
    "\n",
    "The Gaussian Naive Bayes classifier is a probabilistic machine learning model particularly suited for continuous data, assuming that the features of each class are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52dcbd-1547-49b3-8f29-2a7c4b8377e4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import joblib\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Convert all values to numerical types, replacing non-numerical values with NaN\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Replace NaNs, infinities, and too large values\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=1000):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Accuracy': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Ensure data is in float64 format\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'var_smoothing': np.logspace(0, -9, num=100)\n",
    "    }\n",
    "\n",
    "    model = GaussianNB()\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=1000)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Accuracy'].append(accuracy)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained Gaussian Naive Bayes model\n",
    "filename = 'model_weights/gaussian_nb_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.KernelExplainer(best_model.predict, X_train_resampled[:100])  # Use a subset of the training data\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaiveBayes_shap_values = pd.DataFrame(shap_values)\n",
    "NaiveBayes_shap_values['Classifier'] = 'NaiveBayesClassifier'\n",
    "NaiveBayes_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "59e4d624a3b76122"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaiveBayes_results = pd.DataFrame(aggregate_metrics)\n",
    "NaiveBayes_results['Model'] = 'NaiveBayesClassifier'\n",
    "NaiveBayes_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "NaiveBayes_results = NaiveBayes_results[['Model',\n",
    "                                                             'Sample',\n",
    "                                                             'Mean CV F1 Score',\n",
    "                                                             'Precision',\n",
    "                                                             'Recall',\n",
    "                                                             'F1 Score',\n",
    "                                                             'Memory (MB) per read mapping',\n",
    "                                                             'Execution Time (s) per read mapping',\n",
    "                                                             'Memory (MB) 50k batch',\n",
    "                                                             'Execution Time (s) 50k batch']]\n",
    "NaiveBayes_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "5cc1f6b683c178c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\" GNB 2 \"\"\"\n",
    "# \n",
    "# import wandb\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import time\n",
    "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from memory_profiler import memory_usage\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Function to preprocess the data\n",
    "# def preprocess_data(X_train, X_test):\n",
    "#     # Convert all values to numerical types, replacing non-numerical values with NaN\n",
    "#     X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce').values\n",
    "#     X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce').values\n",
    "# \n",
    "#     # Replace NaNs, infinities, and too large values\n",
    "#     X_train = np.nan_to_num(X_train, nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "#     X_test = np.nan_to_num(X_test, nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "# \n",
    "#     # Clip values to the range of float32 to avoid overflow\n",
    "#     X_train = np.clip(X_train, np.finfo(np.float64).min, np.finfo(np.float64).max)\n",
    "#     X_test = np.clip(X_test, np.finfo(np.float64).min, np.finfo(np.float64).max)\n",
    "# \n",
    "#     return X_train, X_test\n",
    "# \n",
    "# # Function to process data in batches\n",
    "# def process_in_batches(model, X_test, batch_size=500):\n",
    "#     predictions = []\n",
    "#     total_memory = 0\n",
    "#     total_time = 0\n",
    "# \n",
    "#     for start in range(0, len(X_test), batch_size):\n",
    "#         end = start + batch_size\n",
    "#         batch = X_test[start:end]\n",
    "# \n",
    "#         # Measure memory usage for this batch\n",
    "#         mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "#         total_memory += max(mem_usage) - min(mem_usage)\n",
    "# \n",
    "#         # Measure execution time for this batch\n",
    "#         start_time = time.time()\n",
    "#         preds = model.predict(batch)\n",
    "#         total_time += time.time() - start_time\n",
    "# \n",
    "#         predictions.extend(preds)\n",
    "# \n",
    "#         # Explicitly clear memory\n",
    "#         del batch\n",
    "#         gc.collect()\n",
    "# \n",
    "#     return np.array(predictions), total_memory, total_time\n",
    "# \n",
    "# # Assuming datasets is a list of tuples with training and testing data\n",
    "# datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "#             (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "#             (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "#             (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "# \n",
    "# aggregate_metrics = {'Precision': [],\n",
    "#                      'Recall': [],\n",
    "#                      'F1 Score': [],\n",
    "#                      'Accuracy': [],\n",
    "#                      'Memory (MB) 50k batch': [],\n",
    "#                      'Execution Time (s) 50k batch': [],\n",
    "#                      'Memory (MB) per read mapping': [],\n",
    "#                      'Execution Time (s) per read mapping': []}\n",
    "# \n",
    "# for X_train, X_test, y_train, y_test in datasets:\n",
    "#     # Preprocess the data\n",
    "#     X_train, X_test = preprocess_data(X_train, X_test)\n",
    "# \n",
    "#     # Standardize the data\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "# \n",
    "#     # Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "#     X_train = pca.fit_transform(X_train)\n",
    "#     X_test = pca.transform(X_test)\n",
    "# \n",
    "#     # Ensure data is in float64 format\n",
    "#     X_train = X_train.astype(np.float64)\n",
    "#     X_test = X_test.astype(np.float64)\n",
    "# \n",
    "#     # Convert y_train and y_test to 1D array\n",
    "#     y_train = y_train.squeeze()\n",
    "#     y_test = y_test.squeeze()\n",
    "# \n",
    "#     model = GaussianNB().fit(X_train, y_train)\n",
    "# \n",
    "#     # Process in batches to handle memory usage\n",
    "#     predicted, mem_usage, execution_time = process_in_batches(model, X_test, batch_size=500)\n",
    "# \n",
    "#     # Metrics Calculation\n",
    "#     precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "#     accuracy = accuracy_score(y_test, predicted)\n",
    "# \n",
    "#     execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "#     memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "# \n",
    "#     # Aggregating metrics\n",
    "#     aggregate_metrics['Precision'].append(precision)\n",
    "#     aggregate_metrics['Recall'].append(recall)\n",
    "#     aggregate_metrics['F1 Score'].append(fscore)\n",
    "#     aggregate_metrics['Accuracy'].append(accuracy)\n",
    "#     aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "#     aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "#     aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "#     aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "# \n",
    "# # Save the last trained Gaussian Naive Bayes model\n",
    "# filename = 'model_weights/gaussian_nb_model.joblib'\n",
    "# joblib.dump(model, filename)\n",
    "# \n",
    "# del model, X_test, X_train, y_train, y_test\n",
    "# gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "c46f9dc0ef8f0ac7"
  },
  {
   "cell_type": "markdown",
   "id": "276d7a39-cad7-4d87-8aa7-5d1078b05c35",
   "metadata": {},
   "source": [
    "## Gradient Boosting Algorithms\n",
    "Gradient Boosting Algorithms are a group of powerful machine learning techniques that build predictive models in the form of an ensemble of weak prediction models, typically decision trees. They are known for their effectiveness in handling various types of data and their ability to improve the accuracy of predictions by reducing bias and variance.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Sequential Model Building**: Unlike other techniques that build models in parallel, gradient boosting builds one tree at a time, where each new tree helps to correct errors made by the previously trained tree.\n",
    "\n",
    "2. **Loss Function Optimization**: These algorithms focus on minimizing a loss function iteratively. Each new model incrementally decreases the loss function of the entire system using the gradient descent method.\n",
    "\n",
    "3. **Types of Gradient Boosting**:\n",
    "   - **Gradient Boosting Machines (GBM)**: The traditional form of gradient boosting that sequentially adds predictors and corrects previous models.\n",
    "   - **XGBoost (Extreme Gradient Boosting)**: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.\n",
    "   - **LightGBM**: A gradient boosting framework that uses tree-based learning algorithms, designed for distributed and efficient training, particularly on large datasets.\n",
    "   - **CatBoost**: An algorithm that can handle categorical data naturally and is robust to overfitting, making it particularly effective for a wide range of data science problems.\n",
    "\n",
    "4. **Applications**: Gradient boosting models are used for a wide range of applications, including but not limited to ranking (like search engines), classification, regression, and many other machine learning tasks where high accuracy is desired.\n",
    "\n",
    "5. **Strengths and Limitations**:\n",
    "   - **Strengths**: They are highly accurate, can handle different types of data, and provide feature importance scores, which can be insightful for model interpretation.\n",
    "   - **Limitations**: These models can be prone to overfitting if not tuned properly and are computationally more expensive than simpler models. They also require careful tuning of parameters and aren't as easy to interpret as simpler models.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing gradient boosting models involves careful tuning of parameters like the number of trees, depth of trees, learning rate, and subsample ratio. The choice and tuning of the loss function are also crucial, depending on the specific problem. Due to their complexity, gradient boosting models often require more computational resources and time to train, especially on large datasets.\n",
    "\n",
    "In summary, Gradient Boosting Algorithms are highly effective for complex machine learning problems where predictive accuracy is paramount. Their ability to iteratively correct errors and optimize performance makes them a go-to choice for competitive data science and a wide range of business applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c700d-b374-4b79-b0da-f772bd390b3c",
   "metadata": {},
   "source": [
    "### LGBM classifier\n",
    "LightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of gradient boosting that excels in handling large datasets and high-dimensional features, due to its novel approach of building trees leaf-wise rather than level-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c2872-a46f-4362-af9a-858a62f38989",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import joblib\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Convert all values to numerical types, replacing non-numerical values with NaN\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Replace NaNs, infinities, and too large values\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=500):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Accuracy': [],\n",
    "                     'Memory (MB) 50k batch': [],\n",
    "                     'Execution Time (s) 50k batch': [],\n",
    "                     'Memory (MB) per read mapping': [],\n",
    "                     'Execution Time (s) per read mapping': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # Ensure data is in float32 format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'num_leaves': [31, 50],\n",
    "        'min_data_in_leaf': [20, 50, 100],\n",
    "        'max_depth': [-1, 10, 20],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200]\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier()\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=500)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Accuracy'].append(accuracy)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB) 50k batch'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s) 50k batch'].append(execution_time)\n",
    "    aggregate_metrics['Memory (MB) per read mapping'].append(memory_usage_per_read)\n",
    "    aggregate_metrics['Execution Time (s) per read mapping'].append(execution_time_per_read)\n",
    "\n",
    "# Save the last trained LightGBM model\n",
    "filename = 'model_weights/lgbm_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LGBMClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "LGBMClassifier_shap_values['Classifier'] = 'LGBMClassifier'\n",
    "LGBMClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "6d12fc3734e9d686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LGBMClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "LGBMClassifier_results['Model'] = 'LGBMClassifier'\n",
    "LGBMClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "LGBMClassifier_results = LGBMClassifier_results[['Model',\n",
    "                                                             'Sample',\n",
    "                                                             'Mean CV F1 Score',\n",
    "                                                             'Precision',\n",
    "                                                             'Recall',\n",
    "                                                             'F1 Score',\n",
    "                                                             'Memory (MB) per read mapping',\n",
    "                                                             'Execution Time (s) per read mapping',\n",
    "                                                             'Memory (MB) 50k batch',\n",
    "                                                             'Execution Time (s) 50k batch']]\n",
    "LGBMClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "5de16b28c679cf39"
  },
  {
   "cell_type": "markdown",
   "id": "909659d9-3fbd-460f-8e43-0a61c27d78e9",
   "metadata": {},
   "source": [
    "### XGB Classifier\n",
    "The XGBoost (Extreme Gradient Boosting) Classifier is a highly efficient and scalable implementation of gradient boosting known for its performance and speed, often delivering state-of-the-art results in a wide range of machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746e63a-2e0b-4ae3-bb7a-9944fa289c02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from memory_profiler import memory_usage\n",
    "import gc\n",
    "import shap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Convert all values to numerical types, replacing non-numerical values with NaN\n",
    "    X_train = pd.DataFrame(X_train).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = pd.DataFrame(X_test).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Replace NaNs, infinities, and too large values\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_in_batches(model, X_test, batch_size=500):\n",
    "    predictions = []\n",
    "    total_memory = 0\n",
    "    total_time = 0\n",
    "\n",
    "    for start in range(0, len(X_test), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = X_test[start:end]\n",
    "\n",
    "        # Measure memory usage for this batch\n",
    "        try:\n",
    "            mem_usage = memory_usage((model.predict, (batch,)), interval=0.1)\n",
    "            batch_mem_usage = max(mem_usage) - min(mem_usage)\n",
    "            if batch_mem_usage == 0:\n",
    "                batch_mem_usage = np.nan  # Ensure no zero values\n",
    "        except Exception as e:\n",
    "            batch_mem_usage = np.nan\n",
    "        total_memory += batch_mem_usage\n",
    "\n",
    "        # Measure execution time for this batch\n",
    "        start_time = time.time()\n",
    "        preds = model.predict(batch)\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    return np.array(predictions), total_memory, total_time\n",
    "\n",
    "# Assuming datasets is a list of tuples with training and testing data\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [],\n",
    "                     'Recall': [],\n",
    "                     'F1 Score': [],\n",
    "                     'Accuracy': [],\n",
    "                     'Memory (MB)': [],\n",
    "                     'Execution Time (s)': [],\n",
    "                     'Mean CV F1 Score': []}\n",
    "\n",
    "# Create a placeholder for the best model\n",
    "best_model = None\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Preprocess the data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # Ensure data is in float32 format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    min_samples = min([sum(y_train == cls) for cls in np.unique(y_train)])\n",
    "    if min_samples > 1:\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        smote = SMOTE(k_neighbors=k_neighbors)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If a class has only one sample, duplicate it to allow splitting\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "        for cls in class_counts[class_counts == 1].index:\n",
    "            X_train = np.vstack([X_train, X_train[y_train == cls]])\n",
    "            y_train = np.hstack([y_train, y_train[y_train == cls]])\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    # Ensure the resampled data is in numpy array format\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Cross-validation to detect overfitting\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "    # Adjust the test_size to avoid errors\n",
    "    if len(np.unique(y_train_resampled)) > int(0.2 * len(y_train_resampled)):\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=len(np.unique(y_train_resampled)))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'colsample_bytree': [0.3, 0.7]\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring='f1_weighted', cv=sss)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {e}\")\n",
    "        best_model = model\n",
    "\n",
    "    cv_scores = []\n",
    "    for train_index, test_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "        X_cv_train, X_cv_test = X_train_resampled[train_index], X_train_resampled[test_index]\n",
    "        y_cv_train, y_cv_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "\n",
    "        best_model.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_model.predict(X_cv_test)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_cv_test, y_cv_pred, average='weighted')\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Process in batches to handle memory usage\n",
    "    predicted, mem_usage, execution_time = process_in_batches(best_model, X_test, batch_size=500)\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "\n",
    "    execution_time_per_read = execution_time / len(X_test)  # per read mapping execution time \n",
    "    memory_usage_per_read = mem_usage / len(X_test)  # memory required per read mapping \n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Accuracy'].append(accuracy)\n",
    "    aggregate_metrics['Mean CV F1 Score'].append(np.mean(cv_scores))\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    print({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "# Save the last trained XGBoost model\n",
    "filename = 'model_weights/xgboost_model.joblib'\n",
    "joblib.dump(best_model, filename)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_train_resampled[:100])  # Use a subset of the training data\n",
    "\n",
    "# SHAP summary plot with feature names\n",
    "feature_names = [f\"Feature {i}\" for i in range(X_train_resampled.shape[1])]\n",
    "shap.summary_plot(shap_values, X_train_resampled[:100], feature_names=feature_names)\n",
    "\n",
    "del best_model, X_test, X_train, y_train, y_test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGBClassifier_shap_values = pd.DataFrame(shap_values)\n",
    "XGBClassifier_shap_values['Classifier'] = 'XGBoostClassifier'\n",
    "XGBClassifier_shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "7b99aec3d876d7c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGBClassifier_results = pd.DataFrame(aggregate_metrics)\n",
    "XGBClassifier_results['Model'] = 'XGBoostClassifier'\n",
    "XGBClassifier_results['Sample'] = ['S1', 'S2', 'S3', 'S4']\n",
    "XGBClassifier_results = XGBClassifier_results[['Model',\n",
    "                                                             'Sample',\n",
    "                                                             'Mean CV F1 Score',\n",
    "                                                             'Precision',\n",
    "                                                             'Recall',\n",
    "                                                             'F1 Score',\n",
    "                                                             'Memory (MB) per read mapping',\n",
    "                                                             'Execution Time (s) per read mapping',\n",
    "                                                             'Memory (MB) 50k batch',\n",
    "                                                             'Execution Time (s) 50k batch']]\n",
    "XGBClassifier_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "afd90b1990c8d19c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' Aggregating Gradient Boosting results '''\n",
    "GradientBoosting_results = pd.concat([XGBClassifier_results,\n",
    "                                      LGBMClassifier_results\n",
    "                                      ])\n",
    "GradientBoosting_shap_values = pd.concat([XGBClassifier_shap_values,\n",
    "                                          LGBMClassifier_shap_values\n",
    "                                          ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T23:16:55.644623Z"
    }
   },
   "id": "31bec892cc34870b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
