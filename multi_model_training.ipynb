{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3efc613c-a38d-4b07-845f-4cb6d71b3970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# grid search shrinkage and distance metric for nearest centroid\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import precision_recall_fscore_support as f1_score\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import wandb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from memory_profiler import memory_usage\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from gen_index import GenIndex\n",
    "from io_processing import IO_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c57b1-22dc-469a-a60d-b7923a9abfe2",
   "metadata": {},
   "source": [
    "# Data Processesing\n",
    "This notebook segment outlines the initial steps of preparing a dataset for training a machine learning model, specifically focusing on location-based data. The code is divided into two main parts: data import and preparation of training and test data.\n",
    "\n",
    "## Data Import\n",
    "- The dataset is imported from a CSV file named `training_data.csv` using Pandas.\n",
    "- Any rows with missing values are dropped to ensure data quality and consistency. This is crucial for models that are sensitive to null values.\n",
    "- A specific preprocessing step is included to address an identified issue: rows where the 'org_location' field is 0.0 are removed. This step is noted as necessary due to a peculiar behavior of some models treating 0.0 as a null value.\n",
    "\n",
    "## Preparing Training and Test Data\n",
    "- The dataset is split into features (`x`) and the target variable (`y`), which is 'org_location' in this case.\n",
    "- The data is then divided into training and test sets using the `train_test_split` method from Scikit-Learn. 20% of the data is reserved for testing, and the split is performed with shuffling to ensure randomization, using a random state of 2 for reproducibility.\n",
    "- Further, the training set is split again to create a validation set, also using a 20% split with shuffling and the same random state. This additional split is important for model validation during training.\n",
    "- The training, test, and validation sets are saved into separate CSV files for easy access in later stages of the modeling process.\n",
    "\n",
    "The print statements at the end of each major section provide a quick summary of the dataset sizes, offering a clear understanding of how much data is available for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Pharsing input reference genome\n",
      "\n",
      "pharsing reference sequence reference_samples\\s1.fasta\n",
      "sequence ID: CM001012.3\n",
      "sequence Length: 10000\n",
      "Making rotations\n",
      "Building suffix array column\n",
      "Building BWT\n",
      "Populating FM index character counts\n",
      "Generating k-length seeds\n",
      "Processed s1.fasta, extended, encoded index, and saved as reference_samples\\S1_extended_encoded_reference.csv\n",
      "\n",
      "... Pharsing input reference genome\n",
      "\n",
      "pharsing reference sequence reference_samples\\s2.fasta\n",
      "sequence ID: ref|NC_000021.9|:25851491-26200185\n",
      "sequence Length: 10000\n",
      "Making rotations\n",
      "Building suffix array column\n",
      "Building BWT\n",
      "Populating FM index character counts\n",
      "Generating k-length seeds\n",
      "Processed s2.fasta, extended, encoded index, and saved as reference_samples\\S2_extended_encoded_reference.csv\n",
      "\n",
      "... Pharsing input reference genome\n",
      "\n",
      "pharsing reference sequence reference_samples\\s3.fasta\n",
      "sequence ID: CM007607.1\n",
      "sequence Length: 10000\n",
      "Making rotations\n",
      "Building suffix array column\n",
      "Building BWT\n",
      "Populating FM index character counts\n",
      "Generating k-length seeds\n",
      "Processed s3.fasta, extended, encoded index, and saved as reference_samples\\S3_extended_encoded_reference.csv\n",
      "\n",
      "... Pharsing input reference genome\n",
      "\n",
      "pharsing reference sequence reference_samples\\s4.fasta\n",
      "sequence ID: CM000090.5\n",
      "sequence Length: 10000\n",
      "Making rotations\n",
      "Building suffix array column\n",
      "Building BWT\n",
      "Populating FM index character counts\n",
      "Generating k-length seeds\n",
      "Processed s4.fasta, extended, encoded index, and saved as reference_samples\\S4_extended_encoded_reference.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from io_processing import IO_processing\n",
    "from gen_index import GenIndex\n",
    "\n",
    "# Encoding function\n",
    "def encode(index):\n",
    "    '''encode the data set using 2bit encoding'''\n",
    "    encoded = []\n",
    "    dictionary = {'$': '', ',':'', 'A': '00', 'C': '01', 'G': '10', 'T': '11'}\n",
    "\n",
    "    if isinstance(index, str):\n",
    "        transTable = index.maketrans(dictionary)\n",
    "        txt = index.translate(transTable)\n",
    "        encoded = txt\n",
    "    else:\n",
    "        for row in index:\n",
    "            encoded_row = row.translate(row.maketrans(dictionary))\n",
    "            encoded.append(encoded_row)\n",
    "    return encoded\n",
    "\n",
    "# Function to extend dataset\n",
    "def extend_dataset(df, config):\n",
    "    classes = df['org_location'].unique()\n",
    "    extended_df = pd.DataFrame()\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_df = df[df['org_location'] == cls]\n",
    "        repeat_times = max(1, config['training_iterations'] - cls_df.shape[0])\n",
    "\n",
    "        extended_cls_df = pd.concat([cls_df] * repeat_times, ignore_index=True)\n",
    "        extended_df = pd.concat([extended_df, extended_cls_df], ignore_index=True)\n",
    "\n",
    "    return extended_df\n",
    "\n",
    "# Data import and indexing script\n",
    "config = {\n",
    "    \"seed_length\": 28,\n",
    "    \"read_length\": 100,\n",
    "    \"training_iterations\": 20,\n",
    "}\n",
    "\n",
    "def process_fasta_file(file_path, config):\n",
    "    io_processor = IO_processing()\n",
    "    sequence_id, sequence_string, sequence_length = io_processor.pharse_reference(file_path, config['seed_length'])\n",
    "    return sequence_id, sequence_string, sequence_length\n",
    "\n",
    "def index_sequence(sequence_string, seed_length):\n",
    "    index_obj = GenIndex()\n",
    "    index = index_obj.generate_index(sequence_string, seed_length)\n",
    "    return index\n",
    "\n",
    "def save_index_as_csv(index, file_name):\n",
    "    index_df = pd.DataFrame(index)\n",
    "    index_df.to_csv(file_name, index=False)\n",
    "\n",
    "def encode_dataframe(df):\n",
    "    encoded_df = df.copy()\n",
    "    for column in encoded_df.columns:\n",
    "        if column != 'org_location':\n",
    "            encoded_df[column] = encoded_df[column].apply(lambda x: encode(x) if isinstance(x, str) else x)\n",
    "    return encoded_df\n",
    "\n",
    "folder_path = 'reference_samples'\n",
    "fasta_files = [f for f in os.listdir(folder_path) if f.endswith('.fasta')]\n",
    "\n",
    "for i, fasta_file in enumerate(fasta_files, start=1):\n",
    "    file_path = os.path.join(folder_path, fasta_file)\n",
    "    try:\n",
    "        sequence_id, sequence_string, sequence_length = process_fasta_file(file_path, config)\n",
    "        index = index_sequence(sequence_string, config['seed_length'])\n",
    "        \n",
    "        # Encode the generated index\n",
    "        encoded_index = encode_dataframe(pd.DataFrame(index))\n",
    "\n",
    "        # Extend the dataset\n",
    "        extended_encoded_index = extend_dataset(encoded_index, config)\n",
    "\n",
    "        # Save the extended encoded index\n",
    "        extended_output_file_name = os.path.join(folder_path, f'S{i}_extended_encoded_reference.csv')\n",
    "        save_index_as_csv(extended_encoded_index, extended_output_file_name)\n",
    "\n",
    "        print(f'Processed {fasta_file}, extended, encoded index, and saved as {extended_output_file_name}\\n')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {fasta_file}: {e}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T20:16:23.149431400Z",
     "start_time": "2023-12-14T19:45:20.250589300Z"
    }
   },
   "id": "2da3dfce088320c1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# need to modify the trainign data generation function to include details as listed. "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:37:36.324213300Z",
     "start_time": "2023-12-14T15:37:36.320751700Z"
    }
   },
   "id": "a3926fb5bf6db456"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66569299-5923-42c3-94cc-475d9a88d392",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T20:31:15.845859300Z",
     "start_time": "2023-12-14T20:26:22.300655900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S1_extended_encoded_reference.csv:\n",
      "  X training dataset length: 505548\n",
      "  y training dataset length: 505548\n",
      "Processed S2_extended_encoded_reference.csv:\n",
      "  X training dataset length: 505548\n",
      "  y training dataset length: 505548\n",
      "Processed S3_extended_encoded_reference.csv:\n",
      "  X training dataset length: 505548\n",
      "  y training dataset length: 505548\n",
      "Processed S4_extended_encoded_reference.csv:\n",
      "  X training dataset length: 505548\n",
      "  y training dataset length: 505548\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'reference_samples'\n",
    "reference_files = [f for f in os.listdir(folder_path) if f.endswith('extended_encoded_reference.csv')]\n",
    "\n",
    "for ref_file in reference_files:\n",
    "    # Data import\n",
    "    file_path = os.path.join(folder_path, ref_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna()\n",
    "    df = df[df['org_location'] != 0.0]  # Handle the error with models treating 0.0 as null\n",
    "    df = df.drop(columns=['suffix_array', 'k-seed-extend'])\n",
    "\n",
    "    # Preparing training and test data\n",
    "    x = df.loc[:, df.columns != 'org_location']\n",
    "    y = df.loc[:, 'org_location']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=2)\n",
    "\n",
    "    # Saving the datasets in the same folder\n",
    "    base_filename = ref_file.split('_reference.csv')[0]\n",
    "    X_train.to_csv(os.path.join(folder_path, f\"{base_filename}_x_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(folder_path, f\"{base_filename}_X_test.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(folder_path, f\"{base_filename}_y_train.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(folder_path, f\"{base_filename}_y_test.csv\"), index=False)\n",
    "    X_val.to_csv(os.path.join(folder_path, f\"{base_filename}_X_val.csv\"), index=False)\n",
    "    y_val.to_csv(os.path.join(folder_path, f\"{base_filename}_y_val.csv\"), index=False)\n",
    "\n",
    "    print(f\"Processed {ref_file}:\")\n",
    "    print(f\"  X training dataset length: {len(X_train)}\")\n",
    "    print(f\"  y training dataset length: {len(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "This section of the notebook focuses on the training of multiple machine learning models using Scikit-Learn version 0.24.2. The models selected represent different archetypes in machine learning, including Nearest Neighbour, Averaging Methods, Naïve Bayes, and Gradient Boosting. The specific models and their roles are as follows:\n",
    "\n",
    "## Models Overview\n",
    "\n",
    "1. **Nearest Centroid (Nearest Neighbour Archetype)**: \n",
    "   - This model is a simplistic yet effective approach for classification, based on the concept of the nearest centroid. It can be particularly useful for baseline comparisons.\n",
    "\n",
    "2. **KNeighbours Classifier (Nearest Neighbour Archetype)**: \n",
    "   - A versatile and widely-used model that classifies data based on the closest training examples in the feature space. It's effective for datasets where the decision boundary is irregular.\n",
    "\n",
    "3. **Random Forest (Averaging Methods Archetype)**: \n",
    "   - A robust ensemble learning method, Random Forest constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) of the individual trees.\n",
    "\n",
    "4. **Extra Trees Classifier (Averaging Methods Archetype)**: \n",
    "   - Similar to Random Forest, this model fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve predictive accuracy and control over-fitting.\n",
    "\n",
    "5. **Decision Tree (Averaging Methods Archetype)**: \n",
    "   - A decision tree is a flowchart-like structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
    "\n",
    "6. **Gaussian Naïve Bayes (Naïve Bayes Archetype)**: \n",
    "   - This model applies the Bayes theorem with the “naive” assumption of independence between every pair of features. Gaussian Naïve Bayes is particularly suited when the features have continuous values.\n",
    "\n",
    "7. **LGBM Classifier (Gradient Boosting Archetype)**: \n",
    "   - Light Gradient Boosting Machine is a gradient boosting framework that uses tree-based learning algorithms. It's designed for distributed and efficient training, particularly on large datasets.\n",
    "\n",
    "8. **XGB Classifier (Gradient Boosting Archetype)**: \n",
    "   - XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. It is highly efficient, flexible, and portable, often delivering state-of-the-art performance in many machine learning tasks.\n",
    "\n",
    "## Training Process\n",
    "Each model is trained on the prepared dataset, and the performance is evaluated using the validation set. The training involves tuning model parameters to find the optimal configuration for each model. Key metrics like accuracy, precision, recall, and F1 score are used to assess each model's performance. The diversity in the selected models ensures a comprehensive examination of the dataset, as each model type brings its strengths and weaknesses to different types of data. This approach allows for a thorough understanding of which models are best suited for the specific characteristics of the dataset in question.The results from these models can be used to benchmark performance and guide the selection of the most suitable model for deployment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bca45a8da1be88db"
  },
  {
   "cell_type": "markdown",
   "id": "8a54b8ea-1346-4f8d-864d-95dac9e2ccfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nearest Neighbour\n",
    "Nearest Neighbor models are a fundamental class of algorithms in the field of machine learning, predominantly used for classification tasks, though they can also be employed for regression. These models operate on the principle of similarity, identifying the closest data points in the training set to make predictions for new, unseen data. The simplicity, intuitiveness, and effectiveness of these models make them an essential part of any data scientist's toolkit.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Basic Principle**: The core idea behind nearest neighbor models is that similar data points are close to each other in the feature space. Therefore, the label or value of a new data point can be predicted based on the labels or values of its nearest neighbors in the training set.\n",
    "\n",
    "2. **Distance Metrics**: These models rely on distance metrics to determine the closeness of data points. Common metrics include Euclidean, Manhattan, and Hamming distances. The choice of metric can significantly impact the model's performance and is often dictated by the nature of the data.\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN)**: One of the most popular variants is the K-Nearest Neighbors algorithm. KNN uses a predefined number 'K' to determine the number of neighboring data points to consider for making a prediction. The optimal value of 'K' is typically selected through cross-validation.\n",
    "\n",
    "4. **Nearest Centroid Classifier**: This variant classifies data points based on the closest centroid of the training samples in the feature space. It's particularly useful when the data is well-separated into clusters.\n",
    "\n",
    "5. **Applications**: Nearest neighbor models are used in a wide range of applications, including image recognition, recommendation systems, and medical diagnosis, where the assumption that similar instances have similar outcomes holds true.\n",
    "\n",
    "6. **Strengths and Limitations**:\n",
    "   - **Strengths**: These models are easy to implement, interpret, and don't require assumptions about the underlying data distribution. They're also highly adaptable to changes in the input data.\n",
    "   - **Limitations**: Nearest neighbor models can suffer from high computational costs, especially with large datasets, and can be sensitive to irrelevant or redundant features.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "In practical scenarios, implementing nearest neighbor models involves careful preprocessing of data, selection of an appropriate distance metric, and tuning of parameters like 'K' in KNN. It's also crucial to scale or normalize the data, as these models are sensitive to the scale of the input features.\n",
    "\n",
    "Moreover, modern applications might require optimizations for handling large datasets, such as using approximate nearest neighbor search algorithms or efficient data structures like KD-trees and Ball Trees.\n",
    "\n",
    "In summary, nearest neighbor models are a versatile and straightforward tool for both classification and regression tasks in machine learning. Their ability to adapt to complex, real-world datasets makes them a valuable component of the machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "S1_X_train = pd.read_csv('reference_samples/S1_extended_encoded_X_train.csv')\n",
    "S1_X_test = pd.read_csv('reference_samples/S1_extended_encoded_X_test.csv')\n",
    "S1_y_train = pd.read_csv('reference_samples/S1_extended_encoded_y_train.csv')\n",
    "S1_y_test = pd.read_csv('reference_samples/S1_extended_encoded_y_test.csv')\n",
    "\n",
    "S2_X_train = pd.read_csv('reference_samples/S2_extended_encoded_X_train.csv')\n",
    "S2_X_test = pd.read_csv('reference_samples/S2_extended_encoded_X_test.csv')\n",
    "S2_y_train = pd.read_csv('reference_samples/S2_extended_encoded_y_train.csv')\n",
    "S2_y_test = pd.read_csv('reference_samples/S2_extended_encoded_y_test.csv')\n",
    "\n",
    "S3_X_train = pd.read_csv('reference_samples/S3_extended_encoded_X_train.csv')\n",
    "S3_X_test = pd.read_csv('reference_samples/S3_extended_encoded_X_test.csv')\n",
    "S3_y_train = pd.read_csv('reference_samples/S3_extended_encoded_y_train.csv')\n",
    "S3_y_test = pd.read_csv('reference_samples/S3_extended_encoded_y_test.csv')\n",
    "\n",
    "S4_X_train = pd.read_csv('reference_samples/S4_extended_encoded_X_train.csv')\n",
    "S4_X_test = pd.read_csv('reference_samples/S4_extended_encoded_X_test.csv')\n",
    "S4_y_train = pd.read_csv('reference_samples/S4_extended_encoded_y_train.csv')\n",
    "S4_y_test = pd.read_csv('reference_samples/S4_extended_encoded_y_test.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T20:35:02.939075600Z",
     "start_time": "2023-12-14T20:35:00.703023800Z"
    }
   },
   "id": "91e7614346ef1e2d"
  },
  {
   "cell_type": "markdown",
   "id": "1aa073ab-dbb5-4e87-8fa0-dcbd00967e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nearest Centroid\n",
    "Nearest Centroid model classifies data by assigning each observation to the class of the nearest centroid, simplifying computation and making it particularly effective for large, well-separated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a2a561d-2570-48e4-b3b8-146d1818cdb0",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T21:08:52.154800900Z",
     "start_time": "2023-12-14T21:03:40.540369700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11926993855c41cea41099eb11aa8412"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Tony\\OneDrive - Ulster University (1)\\Phd - FPGA acceleration in genomics\\Publications + Writing\\PhD_reports\\PhD Thesis\\cp3. Seed generation via ML\\research\\PyPair\\wandb\\run-20231214_210340-fj2ds0p8</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/vsio/seed_generation/runs/fj2ds0p8' target=\"_blank\">worthy-field-47</a></strong> to <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">https://wandb.ai/vsio/seed_generation</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/vsio/seed_generation/runs/fj2ds0p8' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/fj2ds0p8</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Aggregate Execution Time (s)</td><td>▁</td></tr><tr><td>Aggregate F1 Score</td><td>▁</td></tr><tr><td>Aggregate Memory (MB)</td><td>▁</td></tr><tr><td>Aggregate Precision</td><td>▁</td></tr><tr><td>Aggregate Recall</td><td>▁</td></tr><tr><td>Execution Time (s)</td><td>█▁▄▂</td></tr><tr><td>F1 Score</td><td>▁▁▁▁</td></tr><tr><td>Memory (MB)</td><td>█▁▁▁</td></tr><tr><td>Precision</td><td>▁▁▁▁</td></tr><tr><td>Recall</td><td>▁▁▁▁</td></tr><tr><td>Test score</td><td>▁▁▁▁</td></tr><tr><td>Training score</td><td>▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Aggregate Execution Time (s)</td><td>27.45161</td></tr><tr><td>Aggregate F1 Score</td><td>1.0</td></tr><tr><td>Aggregate Memory (MB)</td><td>0.0625</td></tr><tr><td>Aggregate Precision</td><td>1.0</td></tr><tr><td>Aggregate Recall</td><td>1.0</td></tr><tr><td>Execution Time (s)</td><td>24.09222</td></tr><tr><td>F1 Score</td><td>1.0</td></tr><tr><td>Memory (MB)</td><td>0.00391</td></tr><tr><td>Precision</td><td>1.0</td></tr><tr><td>Recall</td><td>1.0</td></tr><tr><td>Test score</td><td>100.0</td></tr><tr><td>Training score</td><td>100.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">worthy-field-47</strong> at: <a href='https://wandb.ai/vsio/seed_generation/runs/fj2ds0p8' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/fj2ds0p8</a><br/> View job at <a href='https://wandb.ai/vsio/seed_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzU2ODEwMg==/version_details/v1' target=\"_blank\">https://wandb.ai/vsio/seed_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzU2ODEwMg==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231214_210340-fj2ds0p8\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "['model_weights/centroid_model.joblib']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='multi-model training', tags=['multimodel', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = NearestCentroid(metric='manhattan').fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = abs(mem_usage_end[0] - mem_usage_start[0])\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "    # scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({#'K-Fold mean': np.mean(scores),\n",
    "               #'k-Fold std': np.std(scores),\n",
    "               'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save model (assuming the last trained model is to be saved)\n",
    "filename = 'model_weights/centroid_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4dece-1ac5-4879-b24e-f40f9e2fcfb0",
   "metadata": {},
   "source": [
    "### Nearest Neighbour\n",
    "The Nearest Neighbor algorithm is a simple and intuitive classification method that predicts the label of a new data point based on the most common label among its closest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e099db-8eb1-4106-afa1-f02737aebf11",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T21:52:56.863690800Z",
     "start_time": "2023-12-14T21:51:39.502114900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19be59ee91254f5e9b4ad1bbf40b0793"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Tony\\OneDrive - Ulster University (1)\\Phd - FPGA acceleration in genomics\\Publications + Writing\\PhD_reports\\PhD Thesis\\cp3. Seed generation via ML\\research\\PyPair\\wandb\\run-20231214_215139-8knfudxo</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/vsio/seed_generation/runs/8knfudxo' target=\"_blank\">peach-dream-48</a></strong> to <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">https://wandb.ai/vsio/seed_generation</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/vsio/seed_generation/runs/8knfudxo' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/8knfudxo</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Aggregate Execution Time (s)</td><td>▁</td></tr><tr><td>Aggregate F1 Score</td><td>▁</td></tr><tr><td>Aggregate Memory (MB)</td><td>▁</td></tr><tr><td>Aggregate Precision</td><td>▁</td></tr><tr><td>Aggregate Recall</td><td>▁</td></tr><tr><td>Execution Time (s)</td><td>▁█▃▁</td></tr><tr><td>F1 Score</td><td>▁▁▁▁</td></tr><tr><td>Memory (MB)</td><td>█▁▁▁</td></tr><tr><td>Precision</td><td>▁▁▁▁</td></tr><tr><td>Recall</td><td>▁▁▁▁</td></tr><tr><td>Test score</td><td>▁▁▁▁</td></tr><tr><td>Training score</td><td>▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Aggregate Execution Time (s)</td><td>4.09961</td></tr><tr><td>Aggregate F1 Score</td><td>1.0</td></tr><tr><td>Aggregate Memory (MB)</td><td>11.33105</td></tr><tr><td>Aggregate Precision</td><td>1.0</td></tr><tr><td>Aggregate Recall</td><td>1.0</td></tr><tr><td>Execution Time (s)</td><td>3.87623</td></tr><tr><td>F1 Score</td><td>1.0</td></tr><tr><td>Memory (MB)</td><td>0.99609</td></tr><tr><td>Precision</td><td>1.0</td></tr><tr><td>Recall</td><td>1.0</td></tr><tr><td>Test score</td><td>100.0</td></tr><tr><td>Training score</td><td>100.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">peach-dream-48</strong> at: <a href='https://wandb.ai/vsio/seed_generation/runs/8knfudxo' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/8knfudxo</a><br/> View job at <a href='https://wandb.ai/vsio/seed_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzU2ODEwMg==/version_details/v1' target=\"_blank\">https://wandb.ai/vsio/seed_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMzU2ODEwMg==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231214_215139-8knfudxo\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "['model_weights/knn_model.joblib']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='K-Nearest Neighbors training', tags=['knn', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', weights='uniform').fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = abs(mem_usage_end[0] - mem_usage_start[0])\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained KNN model\n",
    "filename = 'model_weights/knn_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac670d-6178-44e8-82c5-e9c7a329048c",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees, thereby enhancing predictive accuracy and robustness against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598a6e7c-46b5-4600-8000-1b22a36e736a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='Random Forest training', tags=['random_forest', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = RandomForestClassifier().fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = abs(mem_usage_end[0] - mem_usage_start[0])\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained RandomForest model\n",
    "filename = 'model_weights/random_forest_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d921fc-c8e0-49dd-ba29-3f11594f345b",
   "metadata": {},
   "source": [
    "## Averaging Methods\n",
    "Averaging Methods, including algorithms like Random Forest and Ensemble Methods, are a critical class of machine learning models known for their robustness and accuracy. These models work by combining the predictions from multiple models to improve the overall performance, especially in terms of variance reduction.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Basic Principle**: Averaging methods involve training multiple models and combining their predictions. The final output is typically the mean (for regression tasks) or mode (for classification tasks) of the predictions from all models. This approach helps in mitigating the effects of overfitting and improving the model's generalization capabilities.\n",
    "\n",
    "2. **Random Forest**: A popular example of averaging methods, Random Forest builds numerous decision trees and merges their outcomes. It is a form of 'bagging' where each tree is trained on a subset of the data and features, offering a diversified model performance.\n",
    "\n",
    "3. **Ensemble Learning**: Averaging methods are a subset of ensemble learning, where the objective is to combine the strengths of various models to achieve better accuracy and stability. This includes methods like bagging and boosting.\n",
    "\n",
    "4. **Boosting**: Another form of ensemble learning where models are trained sequentially with each model learning from the errors of its predecessors, thus focusing more on the challenging parts of the dataset.\n",
    "\n",
    "5. **Applications**: These methods are extremely versatile and have been successfully applied in various domains, such as finance for risk assessment, healthcare for disease prediction, and natural language processing tasks.\n",
    "\n",
    "6. **Strengths and Limitations**:\n",
    "   - **Strengths**: Averaging methods are known for their high accuracy, ability to handle large datasets and feature spaces, and robustness against overfitting. They also work well with non-linear data.\n",
    "   - **Limitations**: These models can be complex, computationally intensive, and less interpretable compared to simpler models like linear regression or decision trees.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing averaging methods requires selecting the right base models and determining how to combine their predictions effectively. The choice of base models and the method of combination (like voting or averaging) can significantly impact the performance. It is also crucial to ensure diversity among the base models to maximize the benefits of averaging.\n",
    "\n",
    "Additionally, hyperparameter tuning plays a significant role in optimizing these models. Techniques like cross-validation are essential for determining the optimal settings for parameters like the number of trees in a Random Forest or the learning rate in boosting algorithms.\n",
    "\n",
    "In conclusion, averaging methods are a powerful set of tools in the machine learning arsenal, offering enhanced predictive performance and robustness. Their ability to combine multiple models' strengths makes them suitable for a wide range of complex real-world problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dcf3a-69b9-4d80-84bf-c9316a85ee34",
   "metadata": {},
   "source": [
    "### Extra Trees classifier\n",
    "\n",
    "The Extra Trees Classifier is an ensemble machine learning algorithm that operates similarly to a Random Forest but with randomization at the level of individual tree splits, offering increased variance reduction and potentially faster training at the cost of slightly higher bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e40e0655-b30e-4a77-b24b-b61dc510d846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='Extra Trees Classifier training', tags=['extra_trees', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = ExtraTreesClassifier().fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = abs(mem_usage_end[0] - mem_usage_start[0])\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained Extra Trees model\n",
    "filename = 'model_weights/extra_trees_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d9ab7-c123-42d3-a2cc-5cb1fec70011",
   "metadata": {},
   "source": [
    "### Decision trees classifier\n",
    "The Decision Trees Classifier is a versatile and interpretable machine learning algorithm that classifies data by splitting it based on feature values, creating a tree-like model of decisions and their possible consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5961d957-11ae-4cee-9d74-d9245ec93d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='Decision Tree training', tags=['decision_tree', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = abs(mem_usage_end[0] - mem_usage_start[0])\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained Decision Tree model\n",
    "filename = 'model_weights/decision_tree_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d17a83-820c-4494-b819-9ecdeabc3f3d",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "Naïve Bayes algorithms represent a family of simple yet effective probabilistic classifiers based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. Widely used in various applications, they are particularly known for their efficiency and ease of implementation.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Bayesian Theory**: The core principle of Naïve Bayes is Bayes' theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature.\n",
    "\n",
    "2. **Feature Independence**: Naïve Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. This assumption simplifies the computation, hence the term \"naïve.\"\n",
    "\n",
    "3. **Variants of Naïve Bayes**:\n",
    "   - **Gaussian Naïve Bayes**: Assumes that the continuous values associated with each class are distributed according to a Gaussian distribution.\n",
    "   - **Multinomial Naïve Bayes**: Typically used for document classification, where the features are the frequencies of the words or tokens.\n",
    "   - **Bernoulli Naïve Bayes**: Used in binary classification, especially text classification with 'bag of words' model.\n",
    "\n",
    "4. **Applications**: Naïve Bayes classifiers are widely used in spam filtering, sentiment analysis, and document classification. They are also employed in medical diagnosis and weather prediction.\n",
    "\n",
    "5. **Strengths and Limitations**:\n",
    "   - **Strengths**: They are easy to implement, can handle both continuous and discrete data, and perform well in multi-class prediction. When the independence assumption holds, a Naïve Bayes classifier performs better compared to other models and requires much less training data.\n",
    "   - **Limitations**: Their strong feature independence assumptions can lead to poor performance if this assumption does not hold. In practice, they are often outperformed by models like Random Forest or Gradient Boosting.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing Naïve Bayes models involves careful preprocessing of data. For text data, techniques like bag-of-words or TF-IDF are common. Feature scaling is not required as the classifiers are not sensitive to the magnitude of data. Tuning involves choosing the right variant of Naïve Bayes and adjusting parameters like the smoothing factor in Multinomial and Bernoulli Naïve Bayes.\n",
    "\n",
    "In summary, Naïve Bayes classifiers, with their basis in probability theory, offer a straightforward and efficient approach for building fast and scalable machine learning models. Their simplicity and the ability to make probabilistic predictions make them useful, especially in the initial stages of a modeling pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a98b3-76bc-4108-8680-4debc68e0cd9",
   "metadata": {},
   "source": [
    "## Gaussian naive bayes classifier\n",
    "\n",
    "The Gaussian Naive Bayes classifier is a probabilistic machine learning model particularly suited for continuous data, assuming that the features of each class are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab52dcbd-1547-49b3-8f29-2a7c4b8377e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jhzftta6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ade76a25d42158d1d1acf4acc4c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.025 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.047615…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-disco-33</strong> at: <a href='https://wandb.ai/vsio/seed_generation/runs/jhzftta6' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/jhzftta6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230808_005305-jhzftta6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jhzftta6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeec27dd98a64dd9916566e333242b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Tony\\OneDrive - Ulster University (1)\\Phd - FPGA acceleration in genomics\\Publications + Writing\\PhD_reports\\PhD Thesis\\cp3. Seed generation via ML\\research\\wandb\\run-20230808_005423-t0rll461</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vsio/seed_generation/runs/t0rll461' target=\"_blank\">generous-armadillo-34</a></strong> to <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vsio/seed_generation' target=\"_blank\">https://wandb.ai/vsio/seed_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vsio/seed_generation/runs/t0rll461' target=\"_blank\">https://wandb.ai/vsio/seed_generation/runs/t0rll461</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.3 MiB for an array with shape (223979, 6) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m GaussianNB()\n\u001B[0;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m----> 6\u001B[0m predicted \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      8\u001B[0m training_score \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mscore(X_train, y_train) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m      9\u001B[0m test_score \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mscore(X_test, y_test) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:106\u001B[0m, in \u001B[0;36m_BaseNB.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    104\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    105\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X(X)\n\u001B[1;32m--> 106\u001B[0m jll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_joint_log_likelihood(X)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_[np\u001B[38;5;241m.\u001B[39margmax(jll, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:516\u001B[0m, in \u001B[0;36mGaussianNB._joint_log_likelihood\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    514\u001B[0m     jointi \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_prior_[i])\n\u001B[0;32m    515\u001B[0m     n_ij \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_[i, :]))\n\u001B[1;32m--> 516\u001B[0m     n_ij \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(((X \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtheta_[i, :]) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_[i, :]), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    517\u001B[0m     joint_log_likelihood\u001B[38;5;241m.\u001B[39mappend(jointi \u001B[38;5;241m+\u001B[39m n_ij)\n\u001B[0;32m    519\u001B[0m joint_log_likelihood \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(joint_log_likelihood)\u001B[38;5;241m.\u001B[39mT\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 10.3 MiB for an array with shape (223979, 6) and data type float64"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='Gaussian Naive Bayes training', tags=['gaussian_nb', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = GaussianNB().fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = mem_usage_end[0] - mem_usage_start[0]\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained Gaussian Naive Bayes model\n",
    "filename = 'model_weights/gaussian_nb_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d7a39-cad7-4d87-8aa7-5d1078b05c35",
   "metadata": {},
   "source": [
    "## Gradient Boosting Algorithms\n",
    "Gradient Boosting Algorithms are a group of powerful machine learning techniques that build predictive models in the form of an ensemble of weak prediction models, typically decision trees. They are known for their effectiveness in handling various types of data and their ability to improve the accuracy of predictions by reducing bias and variance.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Sequential Model Building**: Unlike other techniques that build models in parallel, gradient boosting builds one tree at a time, where each new tree helps to correct errors made by the previously trained tree.\n",
    "\n",
    "2. **Loss Function Optimization**: These algorithms focus on minimizing a loss function iteratively. Each new model incrementally decreases the loss function of the entire system using the gradient descent method.\n",
    "\n",
    "3. **Types of Gradient Boosting**:\n",
    "   - **Gradient Boosting Machines (GBM)**: The traditional form of gradient boosting that sequentially adds predictors and corrects previous models.\n",
    "   - **XGBoost (Extreme Gradient Boosting)**: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.\n",
    "   - **LightGBM**: A gradient boosting framework that uses tree-based learning algorithms, designed for distributed and efficient training, particularly on large datasets.\n",
    "   - **CatBoost**: An algorithm that can handle categorical data naturally and is robust to overfitting, making it particularly effective for a wide range of data science problems.\n",
    "\n",
    "4. **Applications**: Gradient boosting models are used for a wide range of applications, including but not limited to ranking (like search engines), classification, regression, and many other machine learning tasks where high accuracy is desired.\n",
    "\n",
    "5. **Strengths and Limitations**:\n",
    "   - **Strengths**: They are highly accurate, can handle different types of data, and provide feature importance scores, which can be insightful for model interpretation.\n",
    "   - **Limitations**: These models can be prone to overfitting if not tuned properly and are computationally more expensive than simpler models. They also require careful tuning of parameters and aren't as easy to interpret as simpler models.\n",
    "\n",
    "#### Practical Implementation\n",
    "\n",
    "Implementing gradient boosting models involves careful tuning of parameters like the number of trees, depth of trees, learning rate, and subsample ratio. The choice and tuning of the loss function are also crucial, depending on the specific problem. Due to their complexity, gradient boosting models often require more computational resources and time to train, especially on large datasets.\n",
    "\n",
    "In summary, Gradient Boosting Algorithms are highly effective for complex machine learning problems where predictive accuracy is paramount. Their ability to iteratively correct errors and optimize performance makes them a go-to choice for competitive data science and a wide range of business applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c700d-b374-4b79-b0da-f772bd390b3c",
   "metadata": {},
   "source": [
    "### LGBM classifier\n",
    "LightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of gradient boosting that excels in handling large datasets and high-dimensional features, due to its novel approach of building trees leaf-wise rather than level-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e09c2872-a46f-4362-af9a-858a62f38989",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "exception: access violation writing 0x0000000000000000",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12176\\1082167846.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLGBMClassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mcv_scores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_val_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mcv_s\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcv_scores\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"CV average score: %.2f\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mcv_s\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001B[0m\n\u001B[0;32m    965\u001B[0m                     \u001B[0mvalid_sets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mvalid_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_le\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalid_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    966\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 967\u001B[1;33m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001B[0m\u001B[0;32m    968\u001B[0m                     \u001B[0meval_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0meval_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_sample_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0meval_sample_weight\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    969\u001B[0m                     \u001B[0meval_class_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0meval_class_weight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_init_score\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0meval_init_score\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001B[0m\n\u001B[0;32m    746\u001B[0m         \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mevals_result\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    747\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 748\u001B[1;33m         self._Booster = train(\n\u001B[0m\u001B[0;32m    749\u001B[0m             \u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    750\u001B[0m             \u001B[0mtrain_set\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_set\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001B[0m\n\u001B[0;32m    269\u001B[0m     \u001B[1;31m# construct booster\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    270\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 271\u001B[1;33m         \u001B[0mbooster\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBooster\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_set\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_set\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    272\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mis_valid_contain_train\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    273\u001B[0m             \u001B[0mbooster\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_train_data_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, params, train_set, model_file, model_str, silent)\u001B[0m\n\u001B[0;32m   2608\u001B[0m             \u001B[0mparams_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparam_dict_to_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2609\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mctypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mc_void_p\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2610\u001B[1;33m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001B[0m\u001B[0;32m   2611\u001B[0m                 \u001B[0mtrain_set\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2612\u001B[0m                 \u001B[0mc_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: exception: access violation writing 0x0000000000000000"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='LightGBM training', tags=['lgbm', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = LGBMClassifier().fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = mem_usage_end[0] - mem_usage_start[0]\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained LightGBM model\n",
    "filename = 'model_weights/lgbm_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909659d9-3fbd-460f-8e43-0a61c27d78e9",
   "metadata": {},
   "source": [
    "### XGB Classifier\n",
    "The XGBoost (Extreme Gradient Boosting) Classifier is a highly efficient and scalable implementation of gradient boosting known for its performance and speed, often delivering state-of-the-art results in a wide range of machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746e63a-2e0b-4ae3-bb7a-9944fa289c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='seed_generation', notes='XGBoost training', tags=['xgboost', 'seed generation'])\n",
    "\n",
    "datasets = [(S1_X_train, S1_X_test, S1_y_train, S1_y_test),\n",
    "            (S2_X_train, S2_X_test, S2_y_train, S2_y_test),\n",
    "            (S3_X_train, S3_X_test, S3_y_train, S3_y_test),\n",
    "            (S4_X_train, S4_X_test, S4_y_train, S4_y_test)]\n",
    "\n",
    "aggregate_metrics = {'Precision': [], 'Recall': [], 'F1 Score': [], 'Memory (MB)': [], 'Execution Time (s)': []}\n",
    "\n",
    "for X_train, X_test, y_train, y_test in datasets:\n",
    "    # Convert y_train and y_test to 1D array\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure memory usage\n",
    "    mem_usage_start = memory_usage(-1)\n",
    "    model = XGBClassifier().fit(X_train, y_train)\n",
    "    mem_usage_end = memory_usage(-1)\n",
    "    mem_usage = mem_usage_end[0] - mem_usage_start[0]\n",
    "\n",
    "    predicted = model.predict(X_test)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Metrics Calculation\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'Training score': model.score(X_train, y_train) * 100,\n",
    "               'Test score': model.score(X_test, y_test) * 100,\n",
    "               'Precision': precision,\n",
    "               'Recall': recall,\n",
    "               'F1 Score': fscore,\n",
    "               'Memory (MB)': mem_usage,\n",
    "               'Execution Time (s)': execution_time\n",
    "              })\n",
    "\n",
    "    # Aggregating metrics\n",
    "    aggregate_metrics['Precision'].append(precision)\n",
    "    aggregate_metrics['Recall'].append(recall)\n",
    "    aggregate_metrics['F1 Score'].append(fscore)\n",
    "    aggregate_metrics['Memory (MB)'].append(mem_usage)\n",
    "    aggregate_metrics['Execution Time (s)'].append(execution_time)\n",
    "\n",
    "# Final Aggregation\n",
    "for metric, values in aggregate_metrics.items():\n",
    "    wandb.log({f'Aggregate {metric}': np.mean(values)})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save the last trained XGBoost model\n",
    "filename = 'model_weights/xgboost_model.joblib'\n",
    "joblib.dump(model, filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
